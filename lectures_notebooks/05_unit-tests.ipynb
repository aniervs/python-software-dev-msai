{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1r7LYI2OXxUT"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D_vXthkXxUW"
   },
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "IIbp40gXXxUX",
    "outputId": "47f87ae5-1c9b-4455-dba7-389c3a75a847"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v8/2cskhqjj0_g0zp3824glf1tm0000gn/T/ipykernel_54888/3503852408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should be 6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "nums = [1, 2, 4]\n",
    "\n",
    "assert sum(nums) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPMpNGDCXxUY"
   },
   "source": [
    "Python includes the **unittest** module which provides testing automatization, namely to write preparing and finishing code snippets for all tests, grouping tests, etc.\n",
    "\n",
    "The basic concepts of the **unittest**:\n",
    "\n",
    "**Test fixture** - the preparation is done to run the tests along with any necessary cleanup after the tests. This can include, for example, creating temporary databases or starting a server process.\n",
    "\n",
    "**Test case** - minimal testing block. Checks answers on a given dataset. The unittest module provides the **TestCase** class which can be used to create new test cases.\n",
    "\n",
    "**Test suite** - A group of test cases or a group of several test groups. It's used to combine various tests into one pipeline.\n",
    "\n",
    "**Test runner** - a component that controls test execution and shows the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "XhmNmItwXxUY",
    "outputId": "d584d20e-f5a4-4630-ab68-f03b47e3da6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /Users/isklonin/Library/Jupyter/runtime/kernel-530d8c7e-9ca0-475a-a34d-6c1e00382460 (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/Users/isklonin/Library/Jupyter/runtime/kernel-530d8c7e-9ca0-475a-a34d-6c1e00382460'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/env3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#Example of testing the string methods\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "    # test method names should start with test\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # Verify that s.split won't work for something that is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sQhB6JkDXxUZ"
   },
   "outputs": [],
   "source": [
    "# Let's save last input from cell above\n",
    "\n",
    "def dump_to(path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(_i)  # _i is the \"last executed Input\" in iPython\n",
    "        \n",
    "dump_to('strings.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcsdAxDfXxUZ"
   },
   "source": [
    "## Oops.. Let's make unittest friends with jupyter notebook:\n",
    "\n",
    "Q: But what happened?\n",
    "\n",
    "A: The reason is that unittest.main looks at sys.argv and first parameter is what started IPython or Jupyter, therefore the error about kernel connection file not being a valid attribute. Passing explicit list to unittest.main will prevent IPython and Jupyter look at sys.argv. Passing exit=False will prevent unittest.main to shutdown the kernell process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGAR-4H7XxUa",
    "outputId": "5ce37430-da73-4254-fea2-21b171dc0ab3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x12010ef10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of testing the string methods\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "    # test method names should start with test\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # Verify that s.split won't work for something that is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zf95xlRpXxUb"
   },
   "source": [
    "## Command line interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD2BWipWXxUb",
    "outputId": "bc76040a-672f-4a8d-9358-e3ca815906fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m unittest strings                               # module\n",
    "!python3 -m unittest strings.TestStringMethods             # class\n",
    "!python3 -m unittest strings.TestStringMethods.test_split  # method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxqIXphXXxUc",
    "outputId": "8e7a49b3-104e-4af0-fa5d-c704a3b6f24d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_isupper (strings.TestStringMethods) ... ok\r\n",
      "test_split (strings.TestStringMethods) ... ok\r\n",
      "test_upper (strings.TestStringMethods) ... ok\r\n",
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "#The -v flag provides the more detailed report:\n",
    "!python3 -m unittest -v strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FsuAnRvXxUc"
   },
   "source": [
    "## More flags:\n",
    "\n",
    "-b (--buffer) - the program output on test failure will be shown instead of hidden as usual.\n",
    "\n",
    "-c (--catch) - Ctrl + C, while a test is running, waits for the current test to complete and then reports the current results. Pressing Ctrl + C a second time throws a normal KeyboardInterrupt exception.\n",
    "\n",
    "-f (--failfast) - exit after the first failed test.\n",
    "\n",
    "--locals (starting with Python 3.5) - show local variables for failed tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1Yw9ybcXxUc"
   },
   "source": [
    "## Test detection\n",
    "\n",
    "unittest supports easy test detection. For compatibility with test detection, all test files must be modules or packages imported from the project's top-level directory.\n",
    "\n",
    "Test detection is implemented in TestLoader.discover (), but can be used from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2b05psuXxUc",
    "outputId": "83df96e5-03f0-4066-c679-9886ce58aa52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!mv strings.py test_strings.py  # rename the module to test....py to make it work\n",
    "!python3 -m  unittest  discover\n",
    "\n",
    "#-v (--verbose) - verbose output.\n",
    "#-s (--start-directory) directory_name - test detection start directory (current by default).\n",
    "#-p (--pattern) pattern - test file name template (test*.py by default).\n",
    "#-t (--top-level-directory) directory_name - project top-level directory (default is start-directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function discover in module unittest.loader:\n",
      "\n",
      "discover(self, start_dir, pattern='test*.py', top_level_dir=None)\n",
      "    Find and return all test modules from the specified start\n",
      "    directory, recursing into subdirectories to find them and return all\n",
      "    tests found within them. Only test files that match the pattern will\n",
      "    be loaded. (Using shell style pattern matching.)\n",
      "    \n",
      "    All test modules must be importable from the top level of the project.\n",
      "    If the start directory is not the top level directory then the top\n",
      "    level directory must be specified separately.\n",
      "    \n",
      "    If a test package name (directory with '__init__.py') matches the\n",
      "    pattern then the package will be checked for a 'load_tests' function. If\n",
      "    this exists then it will be called with (loader, tests, pattern) unless\n",
      "    the package has already had load_tests called from the same discovery\n",
      "    invocation, in which case the package module object is not scanned for\n",
      "    tests - this ensures that when a package uses discover to further\n",
      "    discover child tests that infinite recursion does not happen.\n",
      "    \n",
      "    If load_tests exists then discovery does *not* recurse into the package,\n",
      "    load_tests is responsible for loading all tests in the package.\n",
      "    \n",
      "    The pattern is deliberately not stored as a loader attribute so that\n",
      "    packages can continue discovery themselves. top_level_dir is stored so\n",
      "    load_tests does not need to pass this argument in to loader.discover().\n",
      "    \n",
      "    Paths are sorted before being imported to ensure reproducible execution\n",
      "    order even on filesystems with non-alphabetical ordering like ext3/4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unittest import TestLoader\n",
    "help(TestLoader.discover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUtZVSdYXxUd"
   },
   "source": [
    "## Test code organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i5yXy-mYXxUd"
   },
   "outputs": [],
   "source": [
    "# Create some class that will be tested\n",
    "\n",
    "class Widget():\n",
    "    \n",
    "    def __init__(self, name, x = 50, y = 50):\n",
    "        self.name = name\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def size(self):\n",
    "        return (self.x, self.y)\n",
    "    \n",
    "    def resize(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVeymagTXxUd"
   },
   "source": [
    "The basic test blocks are test cases - simple cases that must be checked for correctness.\n",
    "\n",
    "The test case is created by inheriting from unittest.TestCase.\n",
    "\n",
    "Testing code should be self-contained, that is, it should not depend in any way on other tests.\n",
    "\n",
    "The simplest TestCase subclass can simply implement a test method (the method starting with test).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGFF9YDqXxUd",
    "outputId": "917e7a2c-2d79-48bf-e65a-523d467ca467"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1201027c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DefaultWidgetSizeTestCase(unittest.TestCase):\n",
    "    def test_default_widget_size(self):\n",
    "        widget = Widget('The widget')\n",
    "        self.assertEqual(widget.size(), (50, 50))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['',], defaultTest='DefaultWidgetSizeTestCase', exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPFzScmHXxUe"
   },
   "source": [
    "There can be many tests, and some of the configuration code can be repeated. Fortunately, we can define the setup code by implementing a **setUp()** method that will run _before_ each test.\n",
    "\n",
    "We can also define a **tearDown()** method to run _after_ each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fin1zUo8XxUe",
    "outputId": "16e45e2d-ebdd-4157-bb39-0343d44ce58d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x12012aee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleWidgetTestCase(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def test_default_widget_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50, 50),\n",
    "                         'incorrect default size')\n",
    "\n",
    "    def test_widget_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100, 150),\n",
    "                         'wrong size after resize')\n",
    "        \n",
    "    def tearDown(self):\n",
    "        pass\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['',], defaultTest='SimpleWidgetTestCase', exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piUOHbXqXxUe"
   },
   "source": [
    "It is possible to place all tests in the same file as the program itself (such as widgets.py), but placing the tests in a separate file (such as test_widget.py) has many advantages:\n",
    "\n",
    "- The module with the test can be run autonomously from the command line.\n",
    "- Test code can be easily separated from the program.\n",
    "- Less temptation to change tests to match the program code for no apparent reason.\n",
    "- The test code should be changed much less frequently than the program.\n",
    "- Tested code can be more easily refactored.\n",
    "- Tests for C modules should be in separate modules, so why not be consistent?\n",
    "- If the testing strategy changes, there is no need to change the program code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQnwMuhPXxUe"
   },
   "source": [
    "## Test skipping and expected fails\n",
    "\n",
    "unittest supports skipping individual tests as well as test classes. In addition, it supports marking the test as \"not working, but it should be.\"\n",
    "\n",
    "The test is skipped using the **skip()** decorator or one of its conditional forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZghrSbg2XxUe",
    "outputId": "f40e8f51-b440-4a87-f53d-692b272c636f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_format (__main__.MyTestCase) ... skipped 'not supported in this library version'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'requires Windows'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "OK (skipped=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x12012a220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__version__ = (0, 9)\n",
    "# __version__ = (1, 4)\n",
    "\n",
    "platform = \"ubuntu\"\n",
    "# platform = \"windows\"\n",
    "\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    @unittest.skip(\"demonstrating skipping\")\n",
    "    def test_nothing(self):\n",
    "        self.fail(\"shouldn't happen\")\n",
    "\n",
    "    @unittest.skipIf(__version__ < (1, 3),\n",
    "                     \"not supported in this library version\")\n",
    "    def test_format(self):\n",
    "        # Tests that work for only a certain version of the library.\n",
    "        pass\n",
    "\n",
    "    @unittest.skipUnless(platform.startswith(\"win\"), \"requires Windows\")\n",
    "    def test_windows_support(self):\n",
    "        # windows specific testing code\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['', '-v'], defaultTest='MyTestCase', exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDr3cX0pXxUf"
   },
   "source": [
    "#### Classes may be skipped too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMocWzr_XxUf",
    "outputId": "3a8cefa0-778c-46e3-99fa-36f68d869bc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_not_run (__main__.MySkippedTestCase) ... skipped 'showing class skipping'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x12012a3a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@unittest.skip(\"showing class skipping\")\n",
    "class MySkippedTestCase(unittest.TestCase):\n",
    "    def test_not_run(self):\n",
    "        pass\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['', '-v'], defaultTest='MySkippedTestCase', exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1p0CxNlXxUf"
   },
   "source": [
    "#### expectedFailure() is used for expected fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ueH0CmsXxUf",
    "outputId": "4308cae3-4c9d-4455-8bb2-3117ba585f7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fail (__main__.ExpectedFailureTestCase) ... expected failure\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK (expected failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1201506a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExpectedFailureTestCase(unittest.TestCase):\n",
    "    @unittest.expectedFailure\n",
    "    def test_fail(self):\n",
    "        self.assertEqual(1, 0, \"broken\")\n",
    "#         self.assertEqual(0, 0, \"broken\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['', '-v'], defaultTest='ExpectedFailureTestCase', exit=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bLfwbqrXxUf"
   },
   "source": [
    "#### It's very easy to make your own decorator. For example, the following decorator skips the test if the passed object does not have the specified attribute.\n",
    "\n",
    "\n",
    "SetUp() and tearDown() are not triggered for missed tests. SetUpClass() and tearDownClass() are not triggered for missing classes. For missing modules setUpModule() and tearDownModule() are not triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xn64A6Z3XxUf",
    "outputId": "17b3c50e-7626-4948-a70d-6cd855603f44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fail (__main__.YetAnotherTestCase) ... skipped \"[1, 2, 3] doesn't have 'add'\"\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x12012a280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1 = [1, 2, 3]\n",
    "\n",
    "\n",
    "def skipUnlessHasattr(obj, attr):\n",
    "    if hasattr(obj, attr):\n",
    "        return lambda func: func\n",
    "    return unittest.skip(\"{!r} doesn't have {!r}\".format(obj, attr))\n",
    "\n",
    "\n",
    "class YetAnotherTestCase(unittest.TestCase):\n",
    "    @skipUnlessHasattr(obj1, 'add')  # append\n",
    "    def test_fail(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    unittest.main(argv=['', '-v'], defaultTest='YetAnotherTestCase', exit=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B6hX2TCXxUf"
   },
   "source": [
    "#### Hey, what's up with setUpClass() and setUpModule() ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6fP5VVi2XxUf"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class Test(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls._connection = createExpensiveConnectionObject()\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls._connection.destroy()\n",
    "        \n",
    "\n",
    "#These should be implemented as functions:\n",
    "\n",
    "def setUpModule():\n",
    "    createConnection()\n",
    "\n",
    "def tearDownModule():\n",
    "    closeConnection()\n",
    "    \n",
    "\n",
    "# this code isnt executable (for show only), so delete objects\n",
    "del Test, setUpModule, tearDownModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDNKMhJ4XxUf"
   },
   "source": [
    "#### Distinguishing test iterations using subtests\n",
    "\n",
    "When some tests have only minor differences, such as some parameters, unittest allows you to distinguish them within a single test method using the **subTest()** context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0oXrewPXxUf",
    "outputId": "50846f67-4a3d-47da-bfc5-3e7712d2876a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_even (__main__.NumbersTest)\n",
      "Test that numbers between 0 and 3 are all even. ... \n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest) (i=1)\n",
      "Test that numbers between 0 and 3 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v8/2cskhqjj0_g0zp3824glf1tm0000gn/T/ipykernel_54888/2430741492.py\", line 10, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest) (i=3)\n",
      "Test that numbers between 0 and 3 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v8/2cskhqjj0_g0zp3824glf1tm0000gn/T/ipykernel_54888/2430741492.py\", line 10, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x120160880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NumbersTest(unittest.TestCase):\n",
    "\n",
    "    def test_even(self):\n",
    "        \"\"\"\n",
    "        Test that numbers between 0 and 3 are all even.\n",
    "        \"\"\"\n",
    "        for i in range(0, 4):\n",
    "#             self.assertEqual(i % 2, 0)  # or\n",
    "            with self.subTest(i=i):\n",
    "                self.assertEqual(i % 2, 0)\n",
    "                \n",
    "unittest.main(argv=['', '-v'], defaultTest='NumbersTest', exit=False)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mob457hxXxUg"
   },
   "source": [
    "#### Success checks\n",
    "\n",
    "The unittest module provides many functions for a wide variety of tests:\n",
    "\n",
    "```\n",
    "assertEqual(a, b) — a == b\n",
    "\n",
    "assertNotEqual(a, b) — a != b\n",
    "\n",
    "assertTrue(x) — bool(x) is True\n",
    "\n",
    "assertFalse(x) — bool(x) is False\n",
    "\n",
    "assertIs(a, b) — a is b\n",
    "\n",
    "assertIsNot(a, b) — a is not b\n",
    "\n",
    "assertIsNone(x) — x is None\n",
    "\n",
    "assertIsNotNone(x) — x is not None\n",
    "\n",
    "assertIn(a, b) — a in b\n",
    "\n",
    "assertNotIn(a, b) — a not in b\n",
    "\n",
    "assertIsInstance(a, b) — isinstance(a, b)\n",
    "\n",
    "assertNotIsInstance(a, b) — not isinstance(a, b)\n",
    "\n",
    "assertRaises(exc, fun, *args, **kwds) — fun(*args, **kwds) raises exc exception\n",
    "\n",
    "assertRaisesRegex(exc, r, fun, *args, **kwds) — fun(*args, **kwds) throws exc exception and message matches regex r\n",
    "\n",
    "assertWarns(warn, fun, *args, **kwds) — fun(*args, **kwds) raises warning\n",
    "\n",
    "assertWarnsRegex(warn, r, fun, *args, **kwds) — fun(*args, **kwds) raises warning and message matches regex r\n",
    "\n",
    "assertAlmostEqual(a, b) — round(a-b, 7) == 0\n",
    "\n",
    "assertNotAlmostEqual(a, b) — round(a-b, 7) != 0\n",
    "\n",
    "assertGreater(a, b) — a > b\n",
    "\n",
    "assertGreaterEqual(a, b) — a >= b\n",
    "\n",
    "assertLess(a, b) — a < b\n",
    "\n",
    "assertLessEqual(a, b) — a <= b\n",
    "\n",
    "assertRegex(s, r) — r.search(s)\n",
    "\n",
    "assertNotRegex(s, r) — not r.search(s)\n",
    "\n",
    "assertCountEqual(a, b) — a & b contain the same elements in the same quantities, but the order is not important\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD_E54f9XxUg"
   },
   "source": [
    "#### To customize the execution of given tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbTsPPEKXxUg",
    "outputId": "eb07dec7-e548-4287-9666-9c5154d58298"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_default_widget_size (__main__.SimpleWidgetTestCase) ... ok\n",
      "test_widget_resize (__main__.SimpleWidgetTestCase) ... ok\n",
      "test_even (__main__.NumbersTest)\n",
      "Test that numbers between 0 and 3 are all even. ... test_fail (__main__.YetAnotherTestCase) ... skipped \"[1, 2, 3] doesn't have 'add'\"\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest) (i=1)\n",
      "Test that numbers between 0 and 3 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v8/2cskhqjj0_g0zp3824glf1tm0000gn/T/ipykernel_54888/2430741492.py\", line 10, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest) (i=3)\n",
      "Test that numbers between 0 and 3 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v8/2cskhqjj0_g0zp3824glf1tm0000gn/T/ipykernel_54888/2430741492.py\", line 10, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.005s\n",
      "\n",
      "FAILED (failures=2, skipped=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=2>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MySuite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(SimpleWidgetTestCase('test_default_widget_size'))\n",
    "    suite.addTest(SimpleWidgetTestCase('test_widget_resize'))\n",
    "    suite.addTest(NumbersTest('test_even'))\n",
    "    suite.addTest(YetAnotherTestCase('test_fail'))\n",
    "    return suite\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(MySuite())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "unit-tests",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
